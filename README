Dear Reader, in this README-file we go through the code step-by-step and explain what happens and which choices are made when writing the code.

Model.py connects all the separate Python-files.
____________________________________________________________________________________________________________________

First the dataset is read in via get_data (located in data_processing.py). It reads in the dataframe specified in constants.py.
Preprocessing of the dataset is mostly done using Jupyter Notebook. get_data creates a dataframe containing the news article, radio bodytext, and the NA_index.

The number of generated radio messages is specified via n_g_r.
The number of times the evaluation prompt is ran over the generated radio messages is specified via n_s.

_____________________________________________________________________________________________________________________

The dataframe is extended so that it includes the prompts to generate the radio messages.
In create_prompt_newsarticle, the prompts are created using the function generate_prompts_clavie. It outputs three prompts. The first one includes the system prompt and the first user prompt. To the second prompt the news article is added. The third prompt asks to make the output shorter. The three prompts are added to the dataframe.

Asking the LLM to make the output shorter is not used.

_____________________________________________________________________________________________________________________

In the third step, the radio messages are generated using the previously created prompts. It does so row by row. Every generated radio messages is added to its column. 

_____________________________________________________________________________________________________________________

Fourthly, the prompts to evaluate the radio messages are created. The format is based on the Lai et al. paper. It is a simple prompt and forces the LM to give its output in the following way:
    Output 1: 
    Score 1: 
    Uitleg 1: 
Then, the lai prompt is connected to the news article and the 7 generated radio messages + the golden radio message .It does so by saying Nieuwsbericht: "insert news article" Output A: "insert radio-message1" \ Output B: "insert radio-message2' etc. 
For each evaluation aspect this is done and a new column of it is added to the dataframe.

_____________________________________________________________________________________________________________________

Fifthly, the evaluation prompts are used to generate the evaluation scores for the radio messages.They are ran n_s times.
In generate_radio_scores, first a list of the column names is created so that every run can be appended to its own column. 
Then, loop through the dataframe row-by-row and for every row create ls_n_s, which is a list containing for every run a list containing the scores given to the radio messages. Scores belonging to each radio message are placed in a list and added to the corresponding dataframe cell.
The scores are taken using regular expression and are selected based on the presence of integers between Score: and Uitleg.

_____________________________________________________________________________________________________________________

Lastly, post-processing is done. The mean scores of the n_g_r radio messages are calculated. The mean score per aspect and the mean of all four aspects are calculated. Then, the radio messages are ranked based on their mean total score. 

_____________________________________________________________________________________________________________________

Need to make choices in which prompts to run.
Need to think about how many example news articles to run the program on to find answers that are significant (how many examples to run to make it significant)
Need to add possibilities in adding e.g. a news article title to the prompt, or the category. Could make category-dependent prompts.
For now, the golden radio messages are not used. This since adding them has shown in literature that it does not benefit the evaluation results. Are there different ways to utilize them?
Need to compare the quality of standard automatic evaluation metrics (BLUE/ ROUGE etc.) to using the LLM as evaluator?
Need to compare whether what the LLM perceives as high quality is in  correspondance with what humans perceive as high quality.
How is the performance of using different prompts compared? Is it via sample-level, dataset-level? Sample-level is looking at comparing individual radio messages, if Prompt1 gets better results than Prompt2 on most of the radiomessages, Prompt1 is better. Dataset-level looks at the overall dataset. 
Prompt 1 Evaluation Results | Prompt 2 Evaluation Results
RM 1 |              80      |   85
RM 2 |              65      |   70
RM 3 |              90      |   65
RM 4 |              80      |   85
Using Sample-level, Prompt2 is better, since in 3 out of the 4 RMs the quality of the RM is perceived higher.
Using the Dataset-level, Prompt1 is better, since the average evaluation results of all the radio messages is higher.
Need to give room for an editor to add the essence to the news article that he wants, to replace step 3 of the work process.
Takes time to generate the radio messages, so in-production/experiment a solution must be found to make it work.

Build an interaction tool to allow people to give input. Investigate whether the increased human interaction makes the people working with it more willing to accept the outcomes of the model.

Potentially use assert statements to see whether the generated output indeed consists of e.g. Score: if it does not, generate again e.g.

As a try to check whether it works, change in generate_radio_messages and in generate_scores LLM_rm_generator and LLM_rms_evaluator to gpt_generator and gpt_evaluator


Further suggestions?


